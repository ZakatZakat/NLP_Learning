{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/askarembulatov/Github_projects/NLP_task/NLP/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully with ISO-8859-1 encoding.\n",
      "Failed to read file with Windows-1252: 'charmap' codec can't decode byte 0x81 in position 153284: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/sl0l06mn5kv5v699q9j1sgy80000gn/T/ipykernel_11164/1652698398.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['Target'] = data['Sentiment'].replace({value:key for key, value in enumerate(data['Sentiment'].unique())})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "# Try with ISO-8859-1\n",
    "try:\n",
    "    data = pd.read_csv('/Users/askarembulatov/Github_projects/NLP_task/NLP/Notebooks/classification_problem/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
    "    print(\"File read successfully with ISO-8859-1 encoding.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Failed to read file with ISO-8859-1: {e}\")\n",
    "\n",
    "# If the above fails, try with Windows-1252\n",
    "try:\n",
    "    data = pd.read_csv('/Users/askarembulatov/Github_projects/NLP_task/NLP/Notebooks/classification_problem/Corona_NLP_train.csv', encoding='cp1252')\n",
    "    print(\"File read successfully with Windows-1252 encoding.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Failed to read file with Windows-1252: {e}\")\n",
    "\n",
    "data['Target'] = data['Sentiment'].replace({value:key for key, value in enumerate(data['Sentiment'].unique())})\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2305224787363305\n",
      "0.24267314702308626\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=520,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return encoding\n",
    "\n",
    "# Define dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        values = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        embedded = tokenize(values)\n",
    "\n",
    "        input_ids = embedded['input_ids'].flatten()\n",
    "        masks = embedded['attention_mask'].flatten()\n",
    "\n",
    "        return {'input_ids': input_ids, 'masks': masks, 'labels': torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "\n",
    "class SimpleLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(SimpleLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(hn)\n",
    "        \n",
    "        # Apply the ReLU activation function\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Parameters for the model\n",
    "input_size = 520  # The number of features in the input (e.g., size of the embedding vector)\n",
    "hidden_size = 100  # The number of features in the hidden state of the LSTM\n",
    "output_size = 5  # The size of the output, e.g., 1 for a regression task\n",
    "\n",
    "model = SimpleLSTMModel(input_size, hidden_size, output_size, 10).to(device)\n",
    "dataset = TextClassificationDataset(data['OriginalTweet'].values, data['Target'].values, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=10, shuffle=True, drop_last=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "gradient_norms = {}\n",
    "\n",
    "# Функция для инициализации ключей в словаре\n",
    "def init_gradient_norms(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.requires_grad:\n",
    "            gradient_norms[name] = []\n",
    "\n",
    "# Функция для сохранения норм градиентов после каждого обратного распространения\n",
    "def save_gradients(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.grad is not None:\n",
    "            gradient_norms[name].append(parameter.grad.norm().item())\n",
    "\n",
    "init_gradient_norms(model)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Forward pass\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(torch.float32)\n",
    "        attention_mask = batch['masks']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        input_ids, masks, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass и оптимизация\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        save_gradients(model)  # Печать нормы градиентов\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

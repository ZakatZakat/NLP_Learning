{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oskar/NLP/.venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from NLP_Learning import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully with ISO-8859-1 encoding.\n",
      "Failed to read file with Windows-1252: 'charmap' codec can't decode byte 0x81 in position 153284: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143432/1995724205.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['Target'] = data['Sentiment'].replace({value:key for key, value in enumerate(data['Sentiment'].unique())})\n"
     ]
    }
   ],
   "source": [
    "# Try with ISO-8859-1\n",
    "try:\n",
    "    data = pd.read_csv('/home/oskar/NLP/datasets/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
    "    print(\"File read successfully with ISO-8859-1 encoding.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Failed to read file with ISO-8859-1: {e}\")\n",
    "\n",
    "# If the above fails, try with Windows-1252\n",
    "try:\n",
    "    data = pd.read_csv('/home/oskar/NLP/datasets/Corona_NLP_train.csv', encoding='cp1252')\n",
    "    print(\"File read successfully with Windows-1252 encoding.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Failed to read file with Windows-1252: {e}\")\n",
    "\n",
    "data['Target'] = data['Sentiment'].replace({value:key for key, value in enumerate(data['Sentiment'].unique())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model\n",
    "input_size = 520  # The number of features in the input (e.g., size of the embedding vector)\n",
    "hidden_size = 100  # The number of features in the hidden state of the LSTM\n",
    "output_size = 5  # The size of the output, e.g., 1 for a regression task\n",
    "\n",
    "model = SimpleLSTMModel(input_size, hidden_size, output_size, 10).to(device)\n",
    "dataset = TextClassificationDataset(data['OriginalTweet'].values, data['Target'].values, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=10, shuffle=True, drop_last=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "gradient_norms = {}\n",
    "\n",
    "# Функция для инициализации ключей в словаре\n",
    "def init_gradient_norms(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.requires_grad:\n",
    "            gradient_norms[name] = []\n",
    "\n",
    "# Функция для сохранения норм градиентов после каждого обратного распространения\n",
    "def save_gradients(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.grad is not None:\n",
    "            gradient_norms[name].append(parameter.grad.norm().item())\n",
    "\n",
    "init_gradient_norms(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27334143377885783\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    # Forward pass\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(torch.float32)\n",
    "        attention_mask = batch['masks']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        input_ids, masks, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass и оптимизация\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        save_gradients(model)  # Печать нормы градиентов\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfgdsgfsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(1):\n",
    "    # Start timing the epoch\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Initialize counters\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch_start = time.time()\n",
    "\n",
    "        # Loading data to device\n",
    "        input_ids = batch['input_ids'].to(torch.float32)\n",
    "        attention_mask = batch['masks']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        data_load_time = time.time() - batch_start\n",
    "        print(f\"Data Loading Time: {data_load_time:.4f} sec\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(1):\n",
    "    # Start timing the epoch\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Initialize counters\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch_start = time.time()\n",
    "\n",
    "        # Loading data to device\n",
    "        input_ids = batch['input_ids'].to(torch.float32)\n",
    "        attention_mask = batch['masks']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        data_load_time = time.time() - batch_start\n",
    "\n",
    "        # Transfer data to the device (GPU or CPU)\n",
    "        transfer_to_device_start = time.time()\n",
    "        input_ids, masks, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        transfer_to_device_time = time.time() - transfer_to_device_start\n",
    "\n",
    "        # Forward pass\n",
    "        forward_start = time.time()\n",
    "        outputs = model(input_ids).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        forward_time = time.time() - forward_start\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        backward_start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        backward_time = time.time() - backward_start\n",
    "\n",
    "        # Save gradients\n",
    "        save_gradient_start = time.time()\n",
    "        save_gradients(model)  # Save or print gradient norms\n",
    "        save_gradient_time = time.time() - save_gradient_start\n",
    "\n",
    "        # Calculation of metrics\n",
    "        metric_calc_start = time.time()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        metric_calc_time = time.time() - metric_calc_start\n",
    "\n",
    "        # Print time taken for each step\n",
    "        print(f\"Data Loading Time: {data_load_time:.4f} sec\")\n",
    "        print(f\"Transfer to Device Time: {transfer_to_device_time:.4f} sec\")\n",
    "        print(f\"Forward Pass Time: {forward_time:.4f} sec\")\n",
    "        print(f\"Backward Pass Time: {backward_time:.4f} sec\")\n",
    "        print(f\"Save Gradient Time: {save_gradient_time:.4f} sec\")\n",
    "        print(f\"Metric Calculation Time: {metric_calc_time:.4f} sec\")\n",
    "        print(f\"Correct Predictions Count: {correct}\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Total Epoch Time: {epoch_time:.4f} sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
